import numpy as np
import pandas as pd
import os
import jenkspy

from scripts.data_models import DEA
from tqdm import tqdm
from sklearn.preprocessing import MinMaxScaler
from config import govtresponse_config as cfg


master_variables = pd.read_csv(os.path.join(os.getcwd(), cfg.DATA_FOLDER, cfg.INPUT_FILE))
master_variables_copy = master_variables.copy()

# Per capita variables
master_variables['Population_affected_Total'] = master_variables['Population_affected_Total']/master_variables['sum_population']
master_variables['Human_Live_Lost'] = master_variables['Human_Live_Lost']/master_variables['sum_population']

#Commenting out variables that no longer are at RC level
master_variables['Total_Animal_Affected'] = master_variables['Total_Animal_Affected']/master_variables['sum_population']
master_variables['Total_House_Fully_Damaged'] = master_variables['Total_House_Fully_Damaged']/master_variables['sum_population']

master_variables['Crop_Area'] = master_variables['Crop_Area']/master_variables['net_sown_area_in_hac']
master_variables['Roads'] = master_variables['Roads']/master_variables['rc_area']
master_variables['Bridge'] = master_variables['Bridge']/master_variables['rc_area']
master_variables['Embankment breached'] = master_variables['Embankment breached']/master_variables['rc_area']


master_variables['sum_aged_population'] = master_variables['sum_aged_population']/master_variables['rc_area']
master_variables['rail_length'] = master_variables['rail_length']/master_variables['rc_area']
master_variables['schools_count'] = master_variables['schools_count']/master_variables['rc_area']
master_variables['health_centres_count'] = master_variables['health_centres_count']/master_variables['rc_area']
master_variables['road_length'] = master_variables['road_length']/master_variables['rc_area']
master_variables['net_sown_area_in_hac'] = master_variables['net_sown_area_in_hac']/master_variables['rc_area']
master_variables['Embankments affected'] = master_variables['Embankments affected']/master_variables['rc_area']

#INPUT VARS
vulnerability_vars = ["mean_sex_ratio",
                      "schools_count",
                      "health_centres_count",
                      "rail_length",
                      "road_length",
                      "net_sown_area_in_hac",
                      "avg_electricity",
                      "rc_piped_hhds_pct",
                      "rc_nosanitation_hhds_pct",
                      "sum_aged_population",
                      "Embankment breached",
                     ]

#OUTPUT VARS
damage_vars = [#"Total_Animal_Affected","Total_House_Fully_Damaged"
               "Human_Live_Lost","Population_affected_Total", "Crop_Area","Embankments affected",
                 "Roads","Bridge"]


# Apply custom weights based on damage conditions
vulnerability_df = (master_variables[vulnerability_vars + damage_vars + ['timeperiod', 'object_id',]]).copy()

scaler = MinMaxScaler()
# Fit scaler to the data and transform it
vulnerability_df['landd_score'] = scaler.fit_transform(vulnerability_df[damage_vars]).sum(axis=1) + 1

neg_inputs = ["rc_nosanitation_hhds_pct","sum_aged_population","Embankment breached"]

def apply_custom_weights(row, threshold=0.0001):
    # Custom weights multiplier for significant damage
    #custom_weight = row['landd_score']  # Adjust this value as needed
    custom_weight = np.power(row['landd_score'], 2)
    # Check if any damage variable exceeds the threshold
    if any(row[damage_vars] > threshold):
        # Apply custom weights to the outputs (damage vars)
        row[damage_vars + neg_inputs] *= custom_weight

    return row


# Function to assign bins based on breaks
def assign_bin(value, breaks,date,object_id):
    for i in range(len(breaks)):
        if value <= breaks[i]:
            return i + 1  # Since bins start from 1
    return len(breaks)  # If value is greater than the last break

def assign_bin_with_handling(data, n_classes=5):
    try:
        # Perform Natural Jenks classification
        breaks = jenkspy.jenks_breaks(data, n_classes=n_classes)
        
        # Check for duplicate edges
        unique_breaks = np.unique(breaks)
        if len(unique_breaks) < len(breaks):
            # Reduce the number of bins and drop duplicates
            n_classes -= 1
            print(f"Warning : Duplicate bin edges detected. Reducing number of bins to {n_classes}.")
            return assign_bin_with_handling(data, n_classes=n_classes)  # Recursive call with fewer bins
        
        # Return the bins
        return pd.cut(data, bins=unique_breaks, labels=list(range(n_classes, 0, -1)), include_lowest=True)
    
    except ValueError as e:
        print(f"Error during binning: {e}")
        raise


vulnerability_df_months = []
vulnerability_df = vulnerability_df.apply(apply_custom_weights, axis=1)

for month in tqdm(vulnerability_df.timeperiod.unique()):
    print(month)    
    #vulnerability_df = vulnerability_df.apply(apply_custom_weights, axis=1)
    vulnerability_df_month = vulnerability_df[vulnerability_df.timeperiod == month]
    vulnerability_df_month = vulnerability_df_month.set_index('object_id')
    
    # Fit scaler to the data and transform it
    vulnerability_df_month[vulnerability_vars + damage_vars] = scaler.fit_transform(vulnerability_df_month[vulnerability_vars + damage_vars])
  

    # Reverse certain input variables (as more input means more vulnerability)
    vulnerability_df_month['schools_count'] = 1 - vulnerability_df_month['schools_count']
    vulnerability_df_month['health_centres_count'] = 1 - vulnerability_df_month['health_centres_count']
    vulnerability_df_month['rail_length'] = 1 - vulnerability_df_month['rail_length']
    vulnerability_df_month['road_length'] = 1 - vulnerability_df_month['road_length']
    vulnerability_df_month['avg_electricity'] = 1 - vulnerability_df_month['avg_electricity']
    vulnerability_df_month['rc_piped_hhds_pct'] = 1 - vulnerability_df_month['rc_piped_hhds_pct']

    # Reverse all output (damage) variables (as more output means less damage)
    vulnerability_df_month[damage_vars] = 1 - vulnerability_df_month[damage_vars]
    vulnerability_df_month[vulnerability_vars + damage_vars] = np.round(vulnerability_df_month[vulnerability_vars + damage_vars], 8)

    # Input dict
    X = vulnerability_df_month[vulnerability_vars].T.to_dict('list')

    # Output dict
    y = vulnerability_df_month[damage_vars].T.to_dict('list')

    DMU = list(vulnerability_df_month.index)#.astype(int))




    #debug script
    # Debug DEA inputs
    print("\n--- DEA DEBUG ---")
    print("Month:", month)
    print("DMU count:", len(vulnerability_df_month))

    # Check for non-finite values
    bad_mask = ~np.isfinite(vulnerability_df_month[vulnerability_vars + damage_vars])
    if bad_mask.any().any():
        print("Non-finite values detected:")
        print(vulnerability_df_month.loc[bad_mask.any(axis=1), 
            vulnerability_vars + damage_vars])

    # Check zero-variance columns
    for col in vulnerability_vars + damage_vars:
        if vulnerability_df_month[col].std() == 0:
            print(f"No variation in column: {col}")

    # Quick preview of DEA matrix
    print("\nInputs preview:")
    print(vulnerability_df_month[vulnerability_vars].head())

    print("\nOutputs preview:")
    print(vulnerability_df_month[damage_vars].head())

    print("--- END DEBUG ---\n")
            


    df = DEA.CRS(DMU, X, y, orientation="input", dual=False)

    # Merge efficiency results
    vulnerability_df_month = vulnerability_df_month.reset_index().merge(df,
                                                                        left_on='object_id',
                                                                        right_on='DMU')
    vulnerability_df_month['efficiency'] = vulnerability_df_month['efficiency'].astype(float)

    vulnerability_df_month['efficiency'] = np.round(vulnerability_df_month['efficiency'], 6)
    # Perform Natural Jenks classification with 5 classes
    # Handle binning with potential non-unique edges
    try:
        vulnerability_df_month['vulnerability'] = assign_bin_with_handling(
            vulnerability_df_month['efficiency'], n_classes=5
        )
    except ValueError:
        print(f"Skipping month {month} due to binning error.")
        continue

    #vulnerability_df_month.to_csv(os.getcwd()+'/RiskScoreModel/data/vulnerability_df_month.csv', index=False)
    #breaks = jenkspy.jenks_breaks(vulnerability_df_month['efficiency'], n_classes=5)

    '''
    # Add a new column with the assigned bins
    vulnerability_df_month['vulnerability'] = pd.cut(vulnerability_df_month['efficiency'],
                                                     bins=breaks,
                                                     labels=[5, 4, 3, 2, 1],  # Low efficiency = More Vulnerability
                                                     include_lowest=True)
    '''
    vulnerability_df_months.append(vulnerability_df_month)

vulnerability = pd.concat(vulnerability_df_months)
#ulnerability['landd'] = vulnerability[damage_vars].sum(axis=1)

master_variables = master_variables_copy.merge(vulnerability[['timeperiod', 'object_id', 'efficiency', 'vulnerability','landd_score']],
                       on = ['timeperiod', 'object_id'])

master_variables.to_csv(
    os.path.join(os.getcwd(), cfg.DATA_FOLDER, cfg.OUTPUT_FILE),
    index=False
)
